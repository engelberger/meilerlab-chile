/hd1/brownbp1/workspace/bcl/scripts/machine_learning/launch.py -t cross_validation --config-file config.ben.ini -d D4.updated.ready.rand.bin --id D4.updated.ready.rand.1x32_005_025 --local[bcl]
# specify bcl executable
#bcl: /home/mendenjl/qsar_benchmark/bcl-apps-static.exe
bcl: /net/tungsten/hd1/brownbp1/workspace/bcl/build/linux64_release/bin/bcl-apps-static.exe
#bcl: bcl.exe

[main]
# specify whether the bcl executable runs locally, uses pbs, or gnu-parallels for job execution
#local: 20
#host: tungsten
#gparallel: '-j4'
#pbs: '-W group_list=p_meiler'
#gparallel: '--slf ~/nodes_heavy.txt'
max-pbs-jobs: 1400
random_seed:

[variables]
# activity cutoff used in objective functions
cutoff: 0.5
# parity determines whether values smaller than the cutoff are considered active (0) or inactive (1)
parity: 1

# choose one training objective-function
#objective-function: 'RMSD'
#objective-function: 'MAE_NMAD'
objective-function: 'AucRocCurve(cutoff=%(cutoff)s,parity=%(parity)s,x_axis_log=1,min fpr=0.001,max fpr=0.1)'
#objective-function: 'AucRocCurve(cutoff=%(cutoff)s,parity=%(parity)s,x_axis_log=0,min fpr=0,max fpr=1)'
#objective-function: 'BinaryOperation(op=*,lhs=ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=TPR),rhs=ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=TNR))'
#objective-function: 'ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=MCC,adjustable cutoff=True)'
#objective-function: 'EnrichmentAverage(cutoff=%(cutoff)s,enrichment max=0.01,step size=0.00001,parity=%(parity)s)'
#objective-function: 'InformationGainRatio(cutoff=%(cutoff)s,measure='PPV',parity=%(parity)s)'

# Choose misc. training hyperparameters
visdrop: 0.05
hiddrop: 0.25
hiddenneurons: 32
dl_hiddenneurons:512
alpha: 0.5
eta: 0.05
balanceratio: 0.1
droptype: Zero
report_freq: 1

# Choose misc. training hyperparameters for autoencoder
ae_iter: 100
ae_model_path: AutoEncoderModels
ae_hiddrop: 0.25
ae_hiddenneurons: 32

[learning]

#############################
#                           #
# Non-ANN ML base settings  #
#                           #
#############################

#learning-method: 'DecisionTree( objective function=%(objective-function)s,partitioner=Gini,activity cutoff=%(cutoff)s,node score = RatingTimesInitialNumIncorrect,min split = 5)'
#learning-method: 'SupportVectorMachine( objective function=%(objective-function)s, kernel = RBF( gamma=0.4),iterations=500,cost=0.1,gap_threshold=0.1)' 
#learning-method: 'KappaNearestNeighbor( objective function=%(objective-function)s, min kappa=1, max kappa=25)'
#learning-method: 'Kohonen( objective function=%(objective-function)s, map dimensions(20.0,20.0), steps per update=0, length=20, radius=10, neighbor kernel = Gaussian,initializer=RandomlyChosenElements)'
#learning-method: 'OpenclIterateSequentialMinimalOptimization ( objective function=RMSD, gamma=0.4,iterations=500,cost=0.1 )'
#learning-method: 'OpenCLResilientPropagation(objective function=RMSD,steps per call=0, hidden architecture(8))'
#learning-method: 'OpenCLSimplePropagation(hidden architecture(8),eta=0.05,alpha=0.5,steps per call=0,objective function=RMSD)'
#learning-method: 'LinearRegression( objective function=%(objective-function)s,solver=Cholesky(smoothing=0.20))'

#############################
#                           #
#        Shallow ANNs 	    #
#                           #
#############################

#Old reliable classifier
learning-method: 'NeuralNetwork( transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=True,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

#Old reliable regression
#learning-method: 'NeuralNetwork( transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=False,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

#Linear transfer function
#learning-method: 'NeuralNetwork( transfer function = Linear, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=False,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

#Old reliable regression with shallow AutoEncoder - note that these AutoEncoders override the architecture of the subsequent ANN, so they are more like general pre-trainers. For legitimate AutoEncoding, run with 0 iterations on the NeuralNetwork and use the output compacted model with EncodeByModel to turn it into a new input layer. Then, just train with that new bin file using the standard workflow.
#learning-method: 'NeuralNetwork(initial network=AutoEncoder(shuffle=True,balance=False,balance max repeats=100000, transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(ae_hiddrop)s),scaling=AveStd,steps per update=1,hidden architecture(%(ae_hiddenneurons)s),input dropout type=%(droptype)s, iterations=%(ae_iter)s,model storage path=%(ae_model_path)s),transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=False,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

# Old reliable with existing AutoEncoder
#learning-method: 'NeuralNetwork(initial network=File(directory=%(ae_model_path)s,prefix=model),transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=False,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

#############################
#                           #
#        Deep ANNs 	    #
#                           #
#############################

#Ben/Jeff Germany multitasking classifier
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.05,0.05),hidden architecture(256,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(256,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'

#Regression
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(512,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(128,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.25,0.05),hidden architecture(128,32,8), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.25,0.05),hidden architecture(512,128,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.25,0.75,0.05),hidden architecture(64,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(%(dl_hiddenneurons)s,%(hiddenneurons)s), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'


#learning-method: 'NeuralNetwork(balance=False,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.10,0.25,0.05),hidden architecture(1024,64), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=False,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.50,0.10,0.05),hidden architecture(128,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=False,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.90,0.90,0.00),hidden architecture(128,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'

# Regression with shallow AutoEncoder - note that these AutoEncoders override the architecture of the subsequent DNN, so they are more like general pre-trainers. For legitimate AutoEncoding, run with 0 iterations on the NeuralNetwork and use the output compacted model with EncodeByModel to turn it into a new input layer. Then, just train with that new bin file using the standard workflow.
#learning-method: 'NeuralNetwork(initial network=AutoEncoder(shuffle=True,balance=False,balance max repeats=100000, transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(ae_hiddrop)s),scaling=AveStd,steps per update=1,hidden architecture(%(ae_hiddenneurons)s),input dropout type=%(droptype)s, iterations=%(ae_iter)s,model storage path=%(ae_model_path)s),balance=False,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(512,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'

# Regression with existing AutoEncoder
#learning-method: 'NeuralNetwork(initial network=File(directory=%(ae_model_path)s,prefix=model),balance=False,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(512,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'


#############################
#                           #
#        Validation 	    #
#                           #
#############################

# maximum training iterations of chosen learning-method
max-iterations: 2000
monitor-independent-set:
#max_unimproved_iterations: 10
max-minutes: 24000
result_averaging_window: 0
#include-monitoring-in-training:
#swap-training-and-monitoring:
#features: ./feature-scores/Cross_Validation_UtilityTest/rnd26_cols405.obj

# choose one type of training data data assembly 
#balanced:
#combined:

# choose one final-objective-function that is applied on the finalized model
final-objective-function: '%(objective-function)s'
#final-objective-function: 'EnrichmentAverage(cutoff=%(cutoff)s,enrichment max=0.01,step size=0.0001,parity=%(parity)s)'
#final-objective-function: RMSD
#final-objective-function: 'InformationGainRatio(cutoff=%(cutoff)s,measure='PPV',parity=%(parity)s)'

[dataset]
# specify your dataset (.bin), listing just one or multiple datasets is allowed
datasets: ['your.bin']

#features: ../features.original

[score]
# choose one dataset scoring type 
scoring-type: InformationGain
#scoring-type: FScore
#scoring-type: InputSensitivity

# specify one output filename to store the dataset scoring information
output_score_file: score.infogain.out

# specify your features
#features: default_code_input.obj
#features: rnd18_cols294.obj
#features: ./feature-scores/IGR_TEST_PAM/rnd6_cols716.obj
#features: ./feature-scores/IGR_TEST_DiffEq_PAM/rnd16_cols247.obj

[descriptor-selection]
# specify a range [min,max,stepsize] to select the top n evaluated feature columns by score 
range: [10,100,10]

[cv]
monitoring-id-range: [0,4]
independent-id-range: [0,4]
#blind: [4,5]
cross-validations: 5
cv-repeats: 1 
#training-size-limit: 1000000
#monitoring-size-limit: 1000000
override-memory-multiplier: 2.50
#message_level: Verbose

# where to store the model, options = Db, File, or None
store-model: File
id: ChangeMe
#remove-files-on-success:
print-independent-predictions:
show-status:
#features:  ./feature-scores/Cross_Validation_Utility/rnd29_cols314.obj
#features: /home/glickzl/bcl_vhts_qsar_workshop/qsar_cheminfo/1798_QSAR/feature-scores/Cross_Validation/rnd22_cols349.obj

[descriptor-selection-model-dependent]
# sign consistency among the ANN
# higher values give more weight to features that are used in consistent manners among the networks
weight-abs: 0
weight-utility: 0.125
weight-consistency: 0.25
weight-consistency-best: 0.5
#weight-sqr: 0 
attrition-rate: 0.05

#set this flag to use input sensitivity
sensitivity: 
#is-feature-count: 600 
#continue: 
#continue-after-scoring:

#threads to use when calculating input sensitivity
is-threads: 8 

#host to run scoring on
scoring-host: silicon

# minimum # of features to allow (default 1)
#min-features: MIN_FEATURES

# number of descriptor selection rounds to perform
ds-rounds: 150

# min and maximum features to remove per round (default 1 and 100, respectively)
#min-features-removed-per-round: MIN_FEATURES_REMOVED
max-features-removed-per-round: 800

# fraction of features to eliminate each round (bounded by --min-features-removed-per-round and --max-features-removed-per-round)
# defaults to max-features-removed-per-round / initial number of features
#attrition-rate: 0.05

# just as a note - the following listing shows all additional available flags for section [cv]

#training-size-limit TRAIN_SIZE_LIMIT
#monitoring-size-limit MON_SIZE_LIMIT
#monitoring-id-range <min> <max> 
#independent-id-range <min> <max>
#cross-validations <#CV> 
#balanced 
#combined
#features FEATURES
#score-file SCORE_FILE
#top-features TOP_N_FEATURES
#results RESULTS
#final-objective-function <objective-function>
#id <name>
#print-independent-predictions
#max-iterations ITERATIONS
#store-model {Db,File}
#store-metadata {Db,File}
#local [<# of jobs to run simultaneously, default= 12>
#pbs [<extra resource args to qsub>
#gparallel [<arguments to GNU parallels>
#host <hostname>
#max-reruns <RERUNS>
#opencl {Intel,ATI,NVIDIA,AMD,Disable}
#dry-run 
#round ROUND
#pthreads PTHREADS
#override-memory-multiplier: 1.2
memory-offset: 200
#remove-files-on-success
#show-status

#../bcl-all-static.exe model:Train 'NeuralNetwork( transfer function = Sigmoid, weight update = Resilient,dropout(0.0,0.5),objective function = BinaryOperation(op=*,lhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure="TPR"),rhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure="TNR")),data selector=Accuracy(min fraction=1.0,tolerance above=0.4,pure classification = True), steps per update=0,hidden architecture(8), balance=True,balance target ratio=0.5,shuffle=True,initial network=NeuralNetwork( balance=True,balance target ratio=0.5,transfer function = Sigmoid, weight update = Resilient,iterations=110, steps per update=0, hidden architecture(8), shuffle=True,dropout(0.5,0.5)))' -max_minutes 30 -max_iterations 300 -feature_labels ../code_input_scalar_only.obj -opencl Disable --random_seed -final_objective_function 'BinaryOperation(op=*,lhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure=TPR),rhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure=TNR))' -training 'Chunks(number chunks=5,chunks="[0, 5) - [0] - [1]",dataset=Subset(number chunks=5,chunks="[0,5)-[4]",filename="/home/mendenjl/qsar_benchmark/data/1798.randomized.nointerpol.bin")) ' -monitoring 'Chunks(number chunks=5,chunks="[1]",dataset=Subset(number chunks=5,chunks="[0,5)-[4]",filename="/home/mendenjl/qsar_benchmark/data/1798.randomized.nointerpol.bin")) ' -independent 'Chunks(number chunks=5,chunks="[0]",dataset=Subset(number chunks=5,chunks="[0,5)-[4]",filename="/home/mendenjl/qsar_benchmark/data/1798.randomized.nointerpol.bin")) ' 
